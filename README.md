<!--
SPDX-FileCopyrightText: 2024 Deutsche Telekom AG, LlamaIndex, Vercel, Inc.

SPDX-License-Identifier: MIT
-->

# Create TSI

`create-tsi` is a generative AI RAG toolkit that generates AI Applications using [LlamaIndex](https://www.llamaindex.ai/) with low code.

This fork of the original `create-tsi` modifies it to use Open AI LLMs.
AI Applications generated by this fork of `create-tsi`, use LLMs hosted by Open AI.

The purpose of `create-tsi` is to make the AI Application creation process easy, flexible and fast. With `create-tsi` you can generate bots, write agents and customize them for specific use cases.

To build and run this fork of `create-tsi` locally,
ensure you have the project opened in its VS Code Dev Conatiner:

```bash
pnpm install
pnpm run build
cd dist
./index.js
```

Once your app is generated within the `dist` folder, read its `README.md` file to start the app.

## What you'll get

- A Next.js-powered front-end. The app is set up as a chat interface that can answer questions about your data (see below)
- **Python FastAPI** backend: You’ll get a backend powered by the [llama-index python package](https://pypi.org/project/llama-index/)
- The back-end has a single endpoint that allows you to send the state of your chat and receive additional responses

## Using your data

Unless you selected to generate a simple chat, you can supply your own data and the app will index it and be able to answer questions on the data.

Your generated app will have a folder called `data` in the `backend` directory.
The app will ingest any supported files you put in this directory.

## Example

The simplest thing to do is run `create-tsi` in interactive mode:

```bash
pnpm install
pnpm run build
cd dist
./index.js
```

You will be asked for the name of your project, along with other configuration options, something like this:

```bash
>> ./index.js
✔ What is your project named? … my-app
✔ Would you like to generate a NextJS frontend for your FastAPI (Python) backend? … No / Yes
✔ Please provide your T-Systems API key (or reuse TSI_API_KEY env variable): …
✔ Which model would you like to use? › Mixtral-8x7B-Instruct-v0.1
✔ Which embedding model would you like to use? › paraphrase-multilingual-mpnet-base-v2
? Which data source would you like to use? › - Use arrow-keys. Return to submit.
   No data, just a simple chat
❯  Use an example PDF
   Use local files (.pdf, .doc, .docx, .xls, .xlsx, .csv)
   Use local folders
   Use website content (requires Chrome)
   Use data from a database (Mysql)
```

## Code of Conduct

This project has adopted the [Contributor Covenant](https://www.contributor-covenant.org/) in version 2.1 as our code of conduct. Please see the details in our [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md). All contributors must abide by the code of conduct.

By participating in this project, you agree to abide by its [Code of Conduct](./CODE_OF_CONDUCT.md) at all times.

## Licensing

This project follows the [REUSE standard for software licensing](https://reuse.software/).  
Each file contains copyright and license information, and license texts can be found in the [./LICENSES](./LICENSES) folder. For more information visit https://reuse.software/.  
You can find a guide for developers at https://telekom.github.io/reuse-template/.

To annotate your files with licensing information, run:

```shell
pipx run reuse annotate --copyright="Deutsche Telekom AG, LlamaIndex, Vercel, Inc." --license="MIT" --recursive --fallback-dot-license --skip-existing .
```

## LlamaIndex Documentation

- [TS/JS docs](https://ts.llamaindex.ai/)
- [Python docs](https://docs.llamaindex.ai/en/stable/)

Inspired by and adapted from [create-next-app](https://github.com/vercel/next.js/tree/canary/packages/create-next-app)
